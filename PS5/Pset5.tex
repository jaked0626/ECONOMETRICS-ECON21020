% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Problem Set 5},
  pdfauthor={Jake Underland, Groupmates: Ian Bamford, Matthew Chen},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{amsmath}

\title{Problem Set 5}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{ECON 21020 Spring, 2021}
\author{Jake Underland, Groupmates: Ian Bamford, Matthew Chen}
\date{2021-05-19}

\begin{document}
\maketitle

\hypertarget{question-2.}{%
\section{Question 2.}\label{question-2.}}

The OLS problem is: \[\min _ \beta \frac{1}{n}\Sigma(y_i-\beta x_i)^2\]
FOCs: \[\begin{aligned}
\{\beta\}: \; \; &\frac{d}{d\beta} \frac{1}{n}\Sigma(y_i-\beta x_i)^2 = 0\\
\implies& -2\frac{1}{n}\Sigma x_i(y_i-\beta x_i) = 0  \\
\implies& -2 \frac{1}{n}\Sigma x_iy_i + 2 \beta \frac{1}{n}\Sigma x_i^2 = 0 \\
\implies& \hat{\beta}= \frac{\Sigma x_iy_i}{\Sigma x_i^2}
\end{aligned}\] Therefore, \(\hat{\beta}\) is the OLS estimate of
\(\beta\). For bias, \[\begin{aligned} 
\hat{\beta} &=  \frac{\Sigma x_iy_i}{\Sigma x_i^2}\\
E[\hat{\beta}|x_1, \cdots, x_n] &= E[\frac{\Sigma x_iy_i}{\Sigma x_i^2}|x_1, \cdots, x_n]\\ 
&= \frac{\Sigma x_i E[y_i|x_1, \cdots, x_n]}{\Sigma x_i^2}\\
&= \frac{\Sigma x_i E[\beta x_i + u_i|x_1, \cdots, x_n]}{\Sigma x_i^2} \\
&= \frac{\beta \Sigma x_i ^2  + \Sigma x_iE[u_i|x_1, \cdots, x_n]}{\Sigma x_i^2} \\
&= \frac{\beta \Sigma x_i ^2}{\Sigma x_i^2}\\ 
&= \beta  \\
\implies E[\hat{\beta}] &= E[E[\hat{\beta} | x_1, \cdots, x_n]] = \beta
\end{aligned}\] Thus, \(\hat{\beta}\) is unbiased. \[\begin{aligned} 
\bar{\beta} &=  \frac{\Sigma y_i}{\Sigma x_i} \\
E[\bar{\beta}|x_1, \cdots, x_n] &= E[\frac{\Sigma y_i}{\Sigma x_i}|x_1, \cdots, x_n]\\ 
&= \frac{\Sigma E[\beta x_i + u_i|x_1, \cdots, x_n]}{\Sigma x_i}\\
&= \frac{\beta \Sigma x_i  + \Sigma E[u_i|x_1, \cdots, x_n]}{\Sigma x_i} \\
&= \frac{\beta \Sigma x_i}{\Sigma x_i}\\ 
&= \beta \\
\implies E[\bar{\beta}] &= E[E[\bar{\beta} | x_1, \cdots, x_n]] = \beta
\end{aligned}\]

Thus, the second estimator is also unbiased.\\
By the Gauss-Markov theorem, the OLS estimator would be the variance
minimizing estimator under these assumptions. We can check as follows:

\[\begin{aligned}
Var(\hat\beta|X) &= Var(\frac{\Sigma x_iy_i}{\Sigma x_i^2}|X) \\
&\stackrel{MLR\:2}= \frac{\Sigma x_i^2Var(y_i|X)}{(\Sigma x_i^2)^2} \\
&\stackrel{MLR\:5}= \frac{\sigma^2 \Sigma x_i^2}{(\Sigma x_i^2)^2} \\
&= \frac{\sigma^2}{\Sigma x_i^2} \\
\\
Var(\bar\beta|X) &= Var(\frac{\Sigma y_i}{\Sigma x_i}|X) \\
&\stackrel{MLR\:2}= \frac{\Sigma Var(y_i|X)}{(\Sigma x_i)^2} \\
&\stackrel{MLR\:5}= \frac{n\sigma^2}{(\Sigma x_i)^2} \\
&= \frac{n\sigma^2}{(\Sigma x_i)^2} \\
&= \frac{n\sigma^2}{(n \bar x_n)^2} = \frac{\sigma^2}{n\bar{x_n}^2}
\end{aligned}\]

Now, we know from Jensen's inequality that variance is always greater
than or equal to 0:

\[\begin{aligned}
\frac{1}{n}\Sigma x_i^2 - \bar{x_n}^2 &\geq 0 \\
\implies \Sigma x_i^2 - n\bar{x_n}^2 &\geq 0 \dots  \text{ Since $n>0$} \\
\implies \Sigma x_i^2 &\geq n\bar{x_n}^2 \\
\implies \frac{\sigma^2}{\Sigma x_i^2} &\leq \frac{\sigma^2}{n\bar{x_n}^2}
\end{aligned}\]

Thus, we have that the variance of the OLS estimate conditioned on
\(X= x_1, \dots, x_n\) has a smaller variance than the other estimate,
confirming the Gauss-Markov principle.

\hypertarget{question-3.}{%
\section{Question 3.}\label{question-3.}}

\hypertarget{section}{%
\subsection{6.}\label{section}}

\begin{enumerate}
\item[(i)]
Let 
\[\beta \equiv
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{pmatrix},
Y \equiv \begin{pmatrix}
y_1 \\ \vdots \\ y_n 
\end{pmatrix}, X \equiv \begin{pmatrix}
1 & x_{11} & x_{12} & x_{13} \\\vdots &\vdots&\vdots&\vdots \\ x_{n1} & x_{n2} & x_{n3} & x_{n4}
\end{pmatrix}, 
u \equiv \begin{pmatrix}
u_1 \\ \vdots \\ u_n 
\end{pmatrix}\]
Then, $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$ can be written as 
\[Y = X\beta + u\] 
and, as we know from class, the OLS estimator of $\beta$ is 
\[\hat{\beta} = \begin{pmatrix} \hat{\beta_0} \\ \hat\beta_1 \\ \hat\beta_2 \\ \hat\beta_3 \end{pmatrix} = (X'X)^{-1} X' Y\]
\[\begin{aligned} E(\hat\beta|X) = \begin{pmatrix} E(\hat{\beta_0}) \\ E(\hat\beta_1) \\ E(\hat\beta_2) \\ E(\hat\beta_3) \end{pmatrix} &= E((X'X)^{-1}X' Y|X)\\
&=(X'X)^{-1}X'E(Y|X) \\
&= (X'X)^{-1}X'E(X\beta|X) \\
&= (X'X)^{-1}X'XE(\beta|X) \\
&= \beta = \begin{pmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{pmatrix}
\end{aligned}\]
Thus, we have that $E(\hat\beta_1) = \beta_1, E(\hat\beta_2) = \beta_2$. Then, 
\[E(\hat\theta_1) = E(\hat\beta_1 + \hat\beta_2) = E(\hat\beta_1) + E(\hat\beta_2) = \beta_1 + \beta_2 = \theta_1\]
And thus $\hat\theta_1$ is an unbiased estimator of $\theta_1$. 

\item[(ii)]
\[\begin{aligned}
Var(\hat\theta_1) &= Var(\hat\beta_1) + Var(\hat\beta_2) + 2Cov(\hat\beta_1,\hat\beta_2) \\
&=  Var(\hat\beta_1) + Var(\hat\beta_2) + 2 Corr(\hat\beta_1,\hat\beta_2)\sqrt{Var(\hat\beta_1)Var(\hat\beta_2)}\end{aligned}\]  

\end{enumerate}

\hypertarget{section-1}{%
\subsection{11.}\label{section-1}}

From class, we have \[\begin{aligned}
\tilde\beta_1 &=\frac{\Sigma_i \hat r_{i1}y_i}{\Sigma_i \hat r_{i1}^2} \\
&= \frac{\Sigma_i \hat r_{i1}(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + u_i)}{\Sigma_i \hat r_{i1}^2} \\
&= \frac{\beta_0\Sigma_i \hat r_{i1} + \beta_1 \Sigma_i \hat r_{i1}x_{i1} + \beta_2\Sigma_i \hat r_{i1}x_{i2} + \beta_3 \Sigma_i \hat r_{i1}x_{i3} + \Sigma_i \hat r_{i1}u_i}{\Sigma_i \hat r_{i1}^2} \\
&= \beta_1 + \beta_3\frac{\Sigma_i \hat r_{i1}x_{i3}}{\Sigma_i \hat r_{i1}^2} +\frac{\Sigma_i \hat r_{i1}u_i}{\Sigma_i \hat r_{i1}^2}
\end{aligned}\] Where the 4th equality follows from the properties of
the residual of the regression \(x_1\) on \(x_2\): \[\begin{aligned}
\Sigma_i \hat r_{i1} &= 0, \Sigma_i \hat r_{i1}x_{i2} = 0 \\
\Sigma_i \hat r_{i1}x_{i1} &= \Sigma_i \hat r_{i1}(\hat r_{i1} + \hat x_{i1}) \\
&= \Sigma_i \hat r_{i1}^2 + \Sigma_i \hat r_{i1}(\hat\gamma_0 + \hat\gamma_2x_{i2}) \\
&= \Sigma_i \hat r_{i1}^2 + \hat\gamma_0\underbrace{\Sigma_i \hat r_{i1}}_{=0} + \hat\gamma_2\underbrace{\Sigma_i \hat r_{i1}x_{i2}}_{=0}\\
&= \Sigma_i \hat r_{i1}^2
\end{aligned}\] Thus, \[\begin{aligned}
E(\tilde\beta_1|X) &= E(\beta_1 + \beta_3\frac{\Sigma_i \hat r_{i1}x_{i3}}{\Sigma_i \hat r_{i1}^2} +\frac{\Sigma_i \hat r_{i1}u_i}{\Sigma_i \hat r_{i1}^2}|X) \\
&= \beta_1 + \beta_3\frac{\Sigma_i \hat r_{i1}x_{i3}}{\Sigma_i \hat r_{i1}^2} +\frac{\Sigma_i \hat r_{i1}E(u_i|X)}{\Sigma_i \hat r_{i1}^2} \\
&\stackrel{MLR\:4}= \beta_1 + \beta_3\frac{\Sigma_i \hat r_{i1}x_{i3}}{\Sigma_i \hat r_{i1}^2} 
\end{aligned}\]

\hypertarget{c6}{%
\subsection{C6}\label{c6}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(wooldridge)}
\KeywordTok{data}\NormalTok{(wage2)}
\end{Highlighting}
\end{Shaded}

\hypertarget{i}{%
\subsubsection{(i)}\label{i}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(IQ }\OperatorTok{~}\StringTok{ }\NormalTok{educ, }\DataTypeTok{data =}\NormalTok{ wage2)}
\NormalTok{delta_tilde <-}\StringTok{ }\NormalTok{simple1}\OperatorTok{$}\NormalTok{coefficients}
\NormalTok{delta_tilde}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)        educ 
##   53.687154    3.533829
\end{verbatim}

\(\implies \tilde\delta_1 \approx 3.53\)

\hypertarget{ii}{%
\subsubsection{(ii)}\label{ii}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(wage) }\OperatorTok{~}\StringTok{ }\NormalTok{educ, }\DataTypeTok{data =}\NormalTok{ wage2)}
\NormalTok{beta_tilde <-}\StringTok{ }\NormalTok{simple2}\OperatorTok{$}\NormalTok{coefficients}
\NormalTok{beta_tilde}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)        educ 
##  5.97306245  0.05983921
\end{verbatim}

\(\implies \tilde\beta_1 \approx 0.06\)\\
\break

\hypertarget{iii}{%
\subsubsection{(iii)}\label{iii}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multiple <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(wage) }\OperatorTok{~}\StringTok{ }\NormalTok{educ }\OperatorTok{+}\StringTok{ }\NormalTok{IQ, }\DataTypeTok{data =}\NormalTok{ wage2)}
\NormalTok{beta_hats <-}\StringTok{ }\NormalTok{multiple}\OperatorTok{$}\NormalTok{coefficients}
\NormalTok{beta_hats}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)        educ          IQ 
## 5.658287588 0.039119901 0.005863132
\end{verbatim}

\(\implies \hat\beta_1 \approx 0.04, \hat\beta_2 \approx 0.006\)

\hypertarget{iv}{%
\subsubsection{(iv)}\label{iv}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta_hats[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{beta_hats[}\DecValTok{3}\NormalTok{] }\OperatorTok{*}\StringTok{ }\NormalTok{delta_tilde[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       educ 
## 0.05983921
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta_hats[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{beta_hats[}\DecValTok{3}\NormalTok{] }\OperatorTok{*}\StringTok{ }\NormalTok{delta_tilde[}\DecValTok{2}\NormalTok{] }\OperatorTok{==}\StringTok{ }\NormalTok{beta_tilde[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## educ 
## TRUE
\end{verbatim}

\hypertarget{question-4.}{%
\section{Question 4.}\label{question-4.}}

\hypertarget{section-2}{%
\subsection{8.}\label{section-2}}

\hypertarget{i-1}{%
\subsubsection{(i)}\label{i-1}}

\[Var(\hat\beta_1 - 3 \hat\beta_2) = Var(\hat\beta_1) + 9 Var(\hat\beta_2) -6Cov(\hat\beta_1, \hat\beta_2)\]
The standard error is
\[se(\hat\beta_1 - 3 \hat\beta_2) = \sqrt{Var(\hat\beta_1) + 9 Var(\hat\beta_2) -6Cov(\hat\beta_1, \hat\beta_2)}\]

\hypertarget{ii-1}{%
\subsubsection{(ii)}\label{ii-1}}

The t statistic is
\[T = \frac{\hat\beta_1 - 3 \hat\beta_2 - 1}{\sqrt{Var(\hat\beta_1) + 9 Var(\hat\beta_2) -6Cov(\hat\beta_1, \hat\beta_2)}}\]

\hypertarget{iii-1}{%
\subsubsection{(iii)}\label{iii-1}}

\[\begin{aligned}
&\theta_1 = \beta_1 - 3 \beta_2 \\
\implies& \beta_1 = \theta_1 + 3 \beta_2
\end{aligned}\] Thus, \[\begin{aligned}
\theta_1 &= \beta_1 - 3 \beta_2 \\
\implies \beta_1 &= \theta_1 + 3 \beta_2 \\
y &= \beta_0 + (\theta_1 + 3\beta_2)x_1 + \beta_2 x_2 + \beta_3 x_3 + u \\
&= \beta_0 + \theta_1x_1 + \beta_2 (3x_1 +x_2) + \beta_3 x_3 + u 
\end{aligned}\] We can directly obtain \(\hat\theta_1\) and its standard
error by estimating the coefficient and standard error on \(x_1\) in the
above regression.

\hypertarget{c3}{%
\subsection{C3}\label{c3}}

\hypertarget{i-2}{%
\subsubsection{(i)}\label{i-2}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"hprice1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log_price_basic <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(price) }\OperatorTok{~}\StringTok{ }\NormalTok{sqrft }\OperatorTok{+}\StringTok{ }\NormalTok{bdrms, }\DataTypeTok{data =}\NormalTok{ hprice1)}
\NormalTok{log_price_basic}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)       sqrft       bdrms 
## 4.766027213 0.000379446 0.028884467
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta <-}\StringTok{ }\DecValTok{150} \OperatorTok{*}\StringTok{ }\NormalTok{log_price_basic}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{log_price_basic}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{3}\NormalTok{]}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Coefficient of theta is"}\NormalTok{, theta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Coefficient of theta is 0.0858013664032184"
\end{verbatim}

\hypertarget{ii-2}{%
\subsubsection{(ii)}\label{ii-2}}

Since \(\beta_2 = \theta_1 - 150 \beta_1\),
\[\ln(price) = \beta_0 + \beta_1(sqrft - 150 bdrms) + \theta_1 bdrms + u\]

\hypertarget{iii-2}{%
\subsubsection{(iii)}\label{iii-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create sqrft - 150 bdrms}
\NormalTok{hprice1}\OperatorTok{$}\NormalTok{sqrft150bdrms =}\StringTok{ }\NormalTok{hprice1}\OperatorTok{$}\NormalTok{sqrft }\OperatorTok{-}\StringTok{ }\DecValTok{150} \OperatorTok{*}\StringTok{ }\NormalTok{hprice1}\OperatorTok{$}\NormalTok{bdrms}
\NormalTok{log_price_plugged <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(price) }\OperatorTok{~}\StringTok{ }\NormalTok{sqrft150bdrms }\OperatorTok{+}\StringTok{ }\NormalTok{bdrms, }\DataTypeTok{data =}\NormalTok{ hprice1)}
\NormalTok{t_conf_intervals <-}\StringTok{ }\KeywordTok{confint}\NormalTok{(log_price_plugged)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 95 % confidence intervals reported below coefficients, }
\CommentTok{# theta is coefficient of bdrms}
\KeywordTok{stargazer}\NormalTok{(log_price_plugged, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{type =} \StringTok{"latex"}\NormalTok{, }
          \DataTypeTok{ci.custom=}\KeywordTok{list}\NormalTok{(t_conf_intervals))}
\end{Highlighting}
\end{Shaded}

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & log(price) \\ 
\hline \\[-1.8ex] 
 sqrft150bdrms & 0.0004$^{***}$ \\ 
  & (0.0003, 0.0005) \\ 
  & \\ 
 bdrms & 0.086$^{***}$ \\ 
  & (0.033, 0.139) \\ 
  & \\ 
 Constant & 4.766$^{***}$ \\ 
  & (4.573, 4.959) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 88 \\ 
R$^{2}$ & 0.588 \\ 
Adjusted R$^{2}$ & 0.579 \\ 
Residual Std. Error & 0.197 (df = 85) \\ 
F Statistic & 60.729$^{***}$ (df = 2; 85) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

\hypertarget{c5.}{%
\subsection{C5.}\label{c5.}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(equatiomatic)}
\end{Highlighting}
\end{Shaded}

\hypertarget{i-3}{%
\subsubsection{(i)}\label{i-3}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(mlb1)}
\NormalTok{log_salary_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(salary) }\OperatorTok{~}\StringTok{ }\NormalTok{years }\OperatorTok{+}\StringTok{ }\NormalTok{gamesyr }\OperatorTok{+}\StringTok{ }\NormalTok{bavg }\OperatorTok{+}\StringTok{ }\NormalTok{hrunsyr, }\DataTypeTok{data=}\NormalTok{mlb1)}
\KeywordTok{summary}\NormalTok{(log_salary_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log(salary) ~ years + gamesyr + bavg + hrunsyr, 
##     data = mlb1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0642 -0.4614 -0.0271  0.4654  2.7216 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 11.020913   0.265719  41.476  < 2e-16 ***
## years        0.067732   0.012113   5.592 4.55e-08 ***
## gamesyr      0.015759   0.001564  10.079  < 2e-16 ***
## bavg         0.001419   0.001066   1.331    0.184    
## hrunsyr      0.035943   0.007241   4.964 1.08e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.7279 on 348 degrees of freedom
## Multiple R-squared:  0.6254, Adjusted R-squared:  0.6211 
## F-statistic: 145.2 on 4 and 348 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{extract_eq}\NormalTok{(log_salary_}\DecValTok{1}\NormalTok{, }\DataTypeTok{use_coefs =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{raw_tex =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\[
\operatorname{\widehat{log(salary)}} = 11.02 + 0.07(\operatorname{years}) + 0.02(\operatorname{gamesyr}) + 0(\operatorname{bavg}) + 0.04(\operatorname{hrunsyr})
\]

Both statsitical significance and coefficient of \(hrunsyr\) increases.

\hypertarget{ii-3}{%
\subsubsection{(ii)}\label{ii-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log_salary_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(salary) }\OperatorTok{~}\StringTok{ }\NormalTok{years }\OperatorTok{+}\StringTok{ }\NormalTok{gamesyr }\OperatorTok{+}\StringTok{ }\NormalTok{bavg }\OperatorTok{+}\StringTok{ }\NormalTok{hrunsyr}
                   \OperatorTok{+}\StringTok{ }\NormalTok{runsyr }\OperatorTok{+}\StringTok{ }\NormalTok{fldperc }\OperatorTok{+}\StringTok{ }\NormalTok{sbasesyr, }\DataTypeTok{data=}\NormalTok{mlb1)}
\KeywordTok{summary}\NormalTok{(log_salary_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log(salary) ~ years + gamesyr + bavg + hrunsyr + 
##     runsyr + fldperc + sbasesyr, data = mlb1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.11554 -0.44557 -0.08808  0.48731  2.57872 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 10.4082678  2.0032546   5.196 3.50e-07 ***
## years        0.0699848  0.0119756   5.844 1.18e-08 ***
## gamesyr      0.0078995  0.0026775   2.950 0.003391 ** 
## bavg         0.0005296  0.0011038   0.480 0.631656    
## hrunsyr      0.0232106  0.0086392   2.687 0.007566 ** 
## runsyr       0.0173922  0.0050641   3.434 0.000666 ***
## fldperc      0.0010351  0.0020046   0.516 0.605936    
## sbasesyr    -0.0064191  0.0051842  -1.238 0.216479    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.7176 on 345 degrees of freedom
## Multiple R-squared:  0.639,  Adjusted R-squared:  0.6317 
## F-statistic: 87.25 on 7 and 345 DF,  p-value: < 2.2e-16
\end{verbatim}

The factors with the stars next to them are individually statistically
significant.

\hypertarget{iii-3}{%
\subsubsection{(iii)}\label{iii-3}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Result of joint hypotheses F-test}
\KeywordTok{linearHypothesis}\NormalTok{(log_salary_}\DecValTok{2}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"bavg = 0"}\NormalTok{, }\StringTok{"fldperc = 0"}\NormalTok{, }\StringTok{"sbasesyr = 0"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear hypothesis test
## 
## Hypothesis:
## bavg = 0
## fldperc = 0
## sbasesyr = 0
## 
## Model 1: restricted model
## Model 2: log(salary) ~ years + gamesyr + bavg + hrunsyr + runsyr + fldperc + 
##     sbasesyr
## 
##   Res.Df    RSS Df Sum of Sq     F Pr(>F)
## 1    348 178.72                          
## 2    345 177.66  3    1.0583 0.685 0.5617
\end{verbatim}

Since the p-value is .56, we cannot reject the null hypothesis.

\hypertarget{question-5}{%
\section{Question 5}\label{question-5}}

\hypertarget{c3-1}{%
\subsection{C3}\label{c3-1}}

\hypertarget{i-4}{%
\subsubsection{(i)}\label{i-4}}

\[\begin{aligned} 
\log(wage) &= \beta_0 + \beta_1 educ + \beta_2 exper + \beta_3 educ\cdot exper + u \\
&= \beta_0 + (\beta_1 + \beta_3 exper)educ + \beta_2 exper + u
\end{aligned}\]

\hypertarget{ii-4}{%
\subsubsection{(ii)}\label{ii-4}}

\[\begin{aligned}
H_0 &: \; \beta_3 = 0 \\
H_1 &: \; \beta_3 \ne 0
\end{aligned}\]

We use a two-sided test because we cannot rule out the possibility that
experience has a negative effect on the returns to education. For
example, the more experienced a person is, the less significant their
educational background may be in determining wages, as many companies
use education as a signal for abiilty, but experience already works as a
credible signal of one's ability and may lessen the importance of
educational background.

\hypertarget{iii-4}{%
\subsubsection{(iii)}\label{iii-4}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(wage2)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{return_educ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(wage) }\OperatorTok{~}\StringTok{ }\NormalTok{educ }\OperatorTok{+}\StringTok{ }\NormalTok{exper }\OperatorTok{+}\StringTok{ }\NormalTok{educ }\OperatorTok{*}\StringTok{ }\NormalTok{exper, }\DataTypeTok{data =}\NormalTok{ wage2)}
\NormalTok{res <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(return_educ)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log(wage) ~ educ + exper + educ * exper, data = wage2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.88558 -0.24553  0.03558  0.26171  1.28836 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  5.949455   0.240826  24.704   <2e-16 ***
## educ         0.044050   0.017391   2.533   0.0115 *  
## exper       -0.021496   0.019978  -1.076   0.2822    
## educ:exper   0.003203   0.001529   2.095   0.0365 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3923 on 931 degrees of freedom
## Multiple R-squared:  0.1349, Adjusted R-squared:  0.1321 
## F-statistic: 48.41 on 3 and 931 DF,  p-value: < 2.2e-16
\end{verbatim}

Because the two-sided p-value of the interaction is 0.0365, at a 95\%
confidence level, we reject \(H_0\).

\newpage

\hypertarget{iv-1}{%
\subsubsection{(iv)}\label{iv-1}}

By following the hint we get the following equation:

\[\begin{aligned}
&= \beta_0 + \theta_1 educ + \beta_2 exper+ \beta_3educ(exper - 10) + u
\end{aligned}\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create exper-10}
\NormalTok{wage2}\OperatorTok{$}\NormalTok{exper_minus_ten <-}\StringTok{ }\NormalTok{wage2}\OperatorTok{$}\NormalTok{exper }\OperatorTok{-}\StringTok{ }\DecValTok{10}
\NormalTok{return_educ_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(wage) }\OperatorTok{~}\StringTok{ }\NormalTok{educ }\OperatorTok{+}\StringTok{ }\NormalTok{exper }\OperatorTok{+}\StringTok{ }\NormalTok{educ }\OperatorTok{*}\StringTok{ }\NormalTok{(exper_minus_ten), }\DataTypeTok{data =}\NormalTok{ wage2)}
\NormalTok{t_conf_intervals2 <-}\StringTok{ }\KeywordTok{confint}\NormalTok{(return_educ_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 95 % confidence intervals reported below coefficients, }
\CommentTok{# theta is coefficient on educ}
\KeywordTok{stargazer}\NormalTok{(return_educ_}\DecValTok{2}\NormalTok{, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{type =} \StringTok{"latex"}\NormalTok{, }
          \DataTypeTok{ci.custom=}\KeywordTok{list}\NormalTok{(t_conf_intervals2))}
\end{Highlighting}
\end{Shaded}

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & log(wage) \\ 
\hline \\[-1.8ex] 
 educ & 0.076$^{***}$ \\ 
  & (0.063, 0.089) \\ 
  & \\ 
 exper & $-$0.021 \\ 
  & ($-$0.061, 0.018) \\ 
  & \\ 
 exper\_minus\_ten &  \\ 
  &  \\ 
  & \\ 
 educ:exper\_minus\_ten & 0.003$^{**}$ \\ 
  & (0.0002, 0.006) \\ 
  & \\ 
 Constant & 5.949$^{***}$ \\ 
  & (5.477, 6.422) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 935 \\ 
R$^{2}$ & 0.135 \\ 
Adjusted R$^{2}$ & 0.132 \\ 
Residual Std. Error & 0.392 (df = 931) \\ 
F Statistic & 48.407$^{***}$ (df = 3; 931) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

\hypertarget{question-6}{%
\section{Question 6}\label{question-6}}

Two analysts at a bank want to determine an appropriate credit limit for
new customers with a given credit score using existing data on credit
limit decisions. The first analyst studies customers with `good' credit,
and the second studies customers with `excellent' credit. For the
purpose of this exercise, suppose these are the only two categories.
Suppose `Good' credit is scored between 0-400 and `Excellent' credit
400-800. The analysts separately report the following fitted equations:
\[\begin{aligned}
\text{'Good'}&: \hat y_i = 1000 + 0.5 score_i \\
\text{'Excellent'}&: \hat y_i = 1500 + 0.7 score_i
\end{aligned}\]

\begin{enumerate} 
\item[a)] Explain how you could combine the information from both of these univariate regressions by running a single (multivariate) linear regression. Deduce from the information provided what the parameter estimates would be in your multivariate regression and provide a derivation based on the OLS minimization problem.
\newline
\newline
\textit{Solution. }We could combine the the two regressions by introducing a dummy variable. Denote 
\[d_i = \begin{cases}1\text{ if $score_i \geq 400$} \\ 0\text{ if $score_i < 400$} \end{cases}\]
Then, the regression equation would be 
\[ y_i = \beta_0 + \beta_1 score_i + d_i(\gamma + \delta score_i) + \epsilon_i\]
Estimating the parameters via OLS goes as follows: 
\[\begin{aligned}
&\min_{\beta_0, \beta_1, \gamma, \delta} \Sigma_{i=1}^n(y_i-\beta_0 - \beta_1 score_i - d_i(\gamma + \delta score_i))^2 \\
=& \min_{\beta_0, \beta_1, \gamma, \delta} \Sigma_{i:d_i = 0}(y_i-\beta_0 - \beta_1 score_i)^2 + \Sigma_{i:d_i = 1}(y_i-\beta_0 - \beta_1 score_i - \gamma - \delta score_i)^2 \\
=& \min_{\beta_0, \beta_1, \gamma, \delta} \Sigma_{i:d_i = 0}(y_i-\beta_0 - \beta_1 score_i)^2 + \Sigma_{i:d_i = 1}(y_i-(\beta_0 + \gamma) - (\beta_1 + \delta) score_i )^2
\end{aligned}\]
Since the above functions under our assumptions are strictly convex, the minimizations of the first and second summation actually yield the same unique results as the 'Good' regression and 'Excellent' regression, respectively. Thus, the solution is 
\[\begin{aligned}
\hat\beta_0 &= 1000 \\
\hat\gamma &= 500 \\
\hat\beta_1 &= 0.5 \\
\hat\delta &= 0.2
\end{aligned}\]  

\item[b)] What is the estimated size of the ‘jump’? (The ‘jump’ does not occur at 0). What is the estimated ‘kink’?
\newline
\newline
\textit{Solution.} The 'jump' occurs at 400, since that is when the credit scores moves from "Good" to "Excellent". Therefore, the 'jump' is $\gamma + \delta * 400 = 500 + 0.2 * 400 = 580$. The 'kink' is the value of $\delta = 0.2$, which is the increase in the marginal impact of credit score on credit limit decisions.  

\item[c)] Explain how to test the null hypothesis that the intercept and slope for the two credit categories are equal versus the alternative that they differ. What information would you need?
\newline
\newline
\textit{Solution.} 
\[\begin{aligned}
H_0 &: \; \gamma = \delta = 0 \\
H_1 &: \; \gamma \neq 0 \text{ or } \delta \neq 0 
\end{aligned}\]
The restricted and unrestricted regressions are as follows: 
\[\begin{aligned}
\text{Restricted: }\; &y_i \beta_0 + \beta_1 score_i + \epsilon_i\\
\text{Unrestricted: }\; &y_i = \beta_0 + \beta_1 score_i + d_i(\gamma + \delta score_i) + \epsilon_i
\end{aligned}\]
The restricted regression gives us $SSR_R$, the unrestricted regression gives us $SSR_U$. The number of restrictions is 2, and we are estimating 4 parameters.  From here we can compute the $F$-stat, which is 
\[F = \frac{(SSR_R - SSR_U) / 2}{SSR_U/(n - 4 - 1)} = \frac{(SSR_R - SSR_U) / 2}{SSR_U/(n - 5)}\]
To compute this $F$-stat, we would need to run the above regressions and obtain information of $n, SSR_R, SSR_U$. We would reject the null hypothesis if $F > F_{2, n-5, 1-\alpha}$. 

\end{enumerate}

\end{document}
